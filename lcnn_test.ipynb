{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bac2ab-65d3-4056-a671-26aff452790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Process an image with the trained neural network\n",
    "Usage:\n",
    "    demo.py [options] <yaml-config> <checkpoint> <images>...\n",
    "    demo.py (-h | --help )\n",
    "\n",
    "Arguments:\n",
    "   <yaml-config>                 Path to the yaml hyper-parameter file\n",
    "   <checkpoint>                  Path to the checkpoint\n",
    "   <images>                      Path to images\n",
    "\n",
    "Options:\n",
    "   -h --help                     Show this screen.\n",
    "   -d --devices <devices>        Comma seperated GPU devices [default: 0]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import pprint\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import torch\n",
    "import yaml\n",
    "from docopt import docopt\n",
    "\n",
    "import lcnn\n",
    "from lcnn.config import C, M\n",
    "from lcnn.models.line_vectorizer import LineVectorizer\n",
    "from lcnn.models.multitask_learner import MultitaskHead, MultitaskLearner\n",
    "from lcnn.postprocess import postprocess\n",
    "from lcnn.utils import recursive_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb700be7-c698-4d1d-841d-bf081309bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLTOPTS = {\"color\": \"#33FFFF\", \"s\": 15, \"edgecolors\": \"none\", \"zorder\": 5}\n",
    "cmap = plt.get_cmap(\"jet\")\n",
    "norm = mpl.colors.Normalize(vmin=0.9, vmax=1.0)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "\n",
    "def c(x):\n",
    "    return sm.to_rgba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34963f5b-9e29-4621-a093-22b92086b51b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU(s)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#args = docopt(__doc__)\n",
    "# config_file = args[\"<yaml-config>\"] or \"config/wireframe.yaml\"\n",
    "config_file = \"config/wireframe.yaml\"\n",
    "C.update(C.from_yaml(filename=config_file))\n",
    "M.update(C.model)\n",
    "#pprint.pprint(C, indent=4)\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device_name = \"cpu\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args[\"--devices\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(0)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU(s)!\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "device = torch.device(device_name)\n",
    "# checkpoint = torch.load(args[\"<checkpoint>\"], map_location=device)\n",
    "checkpoint = torch.load(\"190418-201834-f8934c6-lr4d10-312k.pth.tar\", map_location=device)\n",
    "\n",
    "# Load model\n",
    "model = lcnn.models.hg(\n",
    "    depth=M.depth,\n",
    "    head=lambda c_in, c_out: MultitaskHead(c_in, c_out),\n",
    "    num_stacks=M.num_stacks,\n",
    "    num_blocks=M.num_blocks,\n",
    "    num_classes=sum(sum(M.head_size, [])),\n",
    ")\n",
    "model = MultitaskLearner(model)\n",
    "model = LineVectorizer(model)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "996eaf82-183b-436a-9781-ab4d2cd0b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/Users/a1222060/abist/obayashi/data/image/MovieCut/\"\n",
    "out_dir = \"/Users/a1222060/abist/obayashi/data/output/MovieCut/moviecut_lcnn/\"\n",
    "imnames = os.listdir(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ab76c58-60b0-44c7-99d1-68f1986dd6a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a1222060\\abist\\obayashi\\Git\\lcnn\\lcnn\\models\\line_vectorizer.py:174: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  y = (index // 128).float() + torch.gather(joff[:, 0], 1, index) + 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1020.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1050.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1080.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1110.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1140.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1170.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame120.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1200.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1230.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1260.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1290.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1320.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1350.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1380.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1410.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1440.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1470.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame150.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1500.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1530.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1560.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1590.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1620.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1650.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1680.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1710.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1740.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1770.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame180.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1800.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1830.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1860.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1890.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1920.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1950.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame1980.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2010.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2040.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2070.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame210.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2100.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2130.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2160.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2190.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2220.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2250.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2280.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2310.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2340.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2370.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame240.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2400.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2430.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2460.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2490.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2520.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2550.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2580.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2610.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2640.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2670.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame270.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2700.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2730.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2760.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2790.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2820.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2850.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2880.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2910.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2940.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame2970.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame30.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame300.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3000.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3030.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3060.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3090.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3120.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3150.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3180.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3210.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3240.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3270.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame330.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3300.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3330.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3360.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3390.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3420.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3450.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3480.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3510.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3540.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3570.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame360.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3600.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3630.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3660.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3690.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3720.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3750.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3780.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3810.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3840.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3870.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame390.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3900.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3930.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3960.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame3990.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4020.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4050.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4080.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4110.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4140.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4170.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame420.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4200.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4230.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4260.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame4290.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame450.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame480.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame510.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame540.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame570.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame60.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame600.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame630.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame660.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame690.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame720.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame750.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame780.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame810.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame840.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame870.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame90.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame900.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame930.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame960.jpg\n",
      "Processing /Users/a1222060/abist/obayashi/data/image/MovieCut/frame990.jpg\n"
     ]
    }
   ],
   "source": [
    "imname = \"/Users/a1222060/abist/obayashi/data/image/iPadPro/IMG_0740.jpg\"\n",
    "for name in imnames:\n",
    "    imname = img_dir + name\n",
    "    print(f\"Processing {imname}\")\n",
    "    im = skimage.io.imread(imname)\n",
    "    if im.ndim == 2:\n",
    "        im = np.repeat(im[:, :, None], 3, 2)\n",
    "    im = im[:, :, :3]\n",
    "    im_resized = skimage.transform.resize(im, (512, 512)) * 255\n",
    "    image = (im_resized - M.image.mean) / M.image.stddev\n",
    "    image = torch.from_numpy(np.rollaxis(image, 2)[None].copy()).float()\n",
    "    with torch.no_grad():\n",
    "        input_dict = {\n",
    "            \"image\": image.to(device),\n",
    "            \"meta\": [\n",
    "                {\n",
    "                    \"junc\": torch.zeros(1, 2).to(device),\n",
    "                    \"jtyp\": torch.zeros(1, dtype=torch.uint8).to(device),\n",
    "                    \"Lpos\": torch.zeros(2, 2, dtype=torch.uint8).to(device),\n",
    "                    \"Lneg\": torch.zeros(2, 2, dtype=torch.uint8).to(device),\n",
    "                }\n",
    "            ],\n",
    "            \"target\": {\n",
    "                \"jmap\": torch.zeros([1, 1, 128, 128]).to(device),\n",
    "                \"joff\": torch.zeros([1, 1, 2, 128, 128]).to(device),\n",
    "            },\n",
    "            \"mode\": \"testing\",\n",
    "        }\n",
    "        H = model(input_dict)[\"preds\"]\n",
    "\n",
    "    lines = H[\"lines\"][0].cpu().numpy() / 128 * im.shape[:2]\n",
    "    scores = H[\"score\"][0].cpu().numpy()\n",
    "    for i in range(1, len(lines)):\n",
    "        if (lines[i] == lines[0]).all():\n",
    "            lines = lines[:i]\n",
    "            scores = scores[:i]\n",
    "            break\n",
    "\n",
    "    # postprocess lines to remove overlapped lines\n",
    "    diag = (im.shape[0] ** 2 + im.shape[1] ** 2) ** 0.5\n",
    "    nlines, nscores = postprocess(lines, scores, diag * 0.01, 0, False)\n",
    "\n",
    "    t = [0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
    "    input_data = cv2.imread(imname)\n",
    "    for l, s in zip(nlines, nscores):\n",
    "        if s>0.98:\n",
    "            x1,y1 = map(int, l[0])\n",
    "            x2,y2 = map(int, l[1])\n",
    "            img_line = cv2.line(input_data, (y1,x1), (y2,x2),(0,0,255),5)\n",
    "    #cv2.imwrite(\"output/moviecut_wireframe/\"+img_name, img_line)\n",
    "    #plt.imshow(img_line)\n",
    "    cv2.imwrite(out_dir+name, img_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a963d-25f4-468d-a514-a1992902ba56",
   "metadata": {},
   "source": [
    "# 閾値の調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "875ec75d-300a-4d2b-adea-c06a7232bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/a1222060/abist/obayashi/data/image/test/IMG_0750.jpg\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/Users/a1222060/abist/obayashi/data/image/test/\"\n",
    "out_dir = \"/Users/a1222060/abist/obayashi/data/output/lcnn/IMG_0750/\"\n",
    "file_name = \"IMG_0750.jpg\"\n",
    "imname = input_dir + file_name\n",
    "#imname = img_dir + name\n",
    "print(f\"Processing {imname}\")\n",
    "im = skimage.io.imread(imname)\n",
    "if im.ndim == 2:\n",
    "    im = np.repeat(im[:, :, None], 3, 2)\n",
    "im = im[:, :, :3]\n",
    "im_resized = skimage.transform.resize(im, (512, 512)) * 255\n",
    "image = (im_resized - M.image.mean) / M.image.stddev\n",
    "image = torch.from_numpy(np.rollaxis(image, 2)[None].copy()).float()\n",
    "with torch.no_grad():\n",
    "    input_dict = {\n",
    "        \"image\": image.to(device),\n",
    "        \"meta\": [\n",
    "            {\n",
    "                \"junc\": torch.zeros(1, 2).to(device),\n",
    "                \"jtyp\": torch.zeros(1, dtype=torch.uint8).to(device),\n",
    "                \"Lpos\": torch.zeros(2, 2, dtype=torch.uint8).to(device),\n",
    "                \"Lneg\": torch.zeros(2, 2, dtype=torch.uint8).to(device),\n",
    "            }\n",
    "        ],\n",
    "        \"target\": {\n",
    "            \"jmap\": torch.zeros([1, 1, 128, 128]).to(device),\n",
    "            \"joff\": torch.zeros([1, 1, 2, 128, 128]).to(device),\n",
    "        },\n",
    "        \"mode\": \"testing\",\n",
    "    }\n",
    "    H = model(input_dict)[\"preds\"]\n",
    "\n",
    "lines = H[\"lines\"][0].cpu().numpy() / 128 * im.shape[:2]\n",
    "scores = H[\"score\"][0].cpu().numpy()\n",
    "for i in range(1, len(lines)):\n",
    "    if (lines[i] == lines[0]).all():\n",
    "        lines = lines[:i]\n",
    "        scores = scores[:i]\n",
    "        break\n",
    "\n",
    "# postprocess lines to remove overlapped lines\n",
    "diag = (im.shape[0] ** 2 + im.shape[1] ** 2) ** 0.5\n",
    "nlines, nscores = postprocess(lines, scores, diag * 0.01, 0, False)\n",
    "\n",
    "#t = [0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.00]\n",
    "threshold = []\n",
    "for i in range(10):\n",
    "    threshold.append(round(1.0-0.01*i, 2))\n",
    "    \n",
    "for i in range(1, 6):\n",
    "    threshold.append(round(1.0-0.1*i, 2))\n",
    "\n",
    "input_data = cv2.imread(imname)\n",
    "\n",
    "for t in threshold:\n",
    "    for l, s in zip(nlines, nscores):\n",
    "        if s>t:\n",
    "            x1,y1 = map(int, l[0])\n",
    "            x2,y2 = map(int, l[1])\n",
    "            img_line = cv2.line(input_data, (y1,x1), (y2,x2),(0,0,255),5)\n",
    "    #cv2.imwrite(\"output/moviecut_wireframe/\"+img_name, img_line)\n",
    "    #plt.imshow(img_line)\n",
    "    cv2.imwrite(out_dir+str(t)+\"_\"+file_name, img_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebfc31-8199-405a-ba56-380d38216091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
